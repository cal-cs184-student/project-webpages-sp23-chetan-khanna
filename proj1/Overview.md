# Overview

I implemented triangle rasterization in this CS 184 project. In other words, I took a virtual drawing (e.g. a cube or a logo) and converted that drawing (that “scene”) into a set of pixel values that I could place in the frame buffer so that they are drawn onto our display.

At first, this was just basic triangle rasterization, which would check whether for each pixel in the frame buffer, a triangle (that we wanted to draw) would be there, and then in that case we would place the appropriate value (a `color`—at first no textures, just a passed in `Color`) for that pixel in the frame buffer.

I then implemented progressively more advanced methods and techniques to perform rasterization. The first improvement I made was supersampling, in which we took multiple samples for each pixel in the frame buffer (so we got multiple colors) and then averaged them out for to get our final `Color` value which we placed for that pixel in the frame buffer. Since this was meant we were sampling the image at a higher frequency, this reduced aliasing.

Subsequently, we implemented the ability to transform, scale, and rotate images. These were done through using the appropriate matrices (e.g. a rotation matrix for rotation).

In order to do implement texturing properly, I had to implement Barycentric coordinates—effectively a coordinate system defined by the three vertices of the triangle, each given a “weight”. This allowed us to perform interpolation properly (hence why an interpolated color triangle was possible), in addition to performing the correct conversion between screen and texture space (since Barycentric coordinates, in a sense, give us our position in the triangle relative to the triangle’s vertices itself).

After this, I implemented texturing, so that rather than filling with triangles with just a solid color, we could sample the `Color` from a `Texture` object for each specific sample we were taking during the rasterization process. This meant that I had to be able to convert to texture space from screen space correctly (i.e. perform the correct transformation) so that we could get the right `Color` for where we were in the texture. There were multiple ways to perform the actual sampling, which I elaborate on below.

Finally, I implemented sampling from different mipmap levels. We want to reduce aliasing by taking a lower-resolution mipmap (i.e. higher level mipmap) for far away objects (since each sample will be much “farther away” when placed in texture space for minified (or far away) textures), and vice versa for closer objects. To do this, as we’re rasterizing, we check the distance between this sample and an adjacent sample (the specific adjacent sample will be elaborated upon below), and, based on that, choose a mipmap level that we will sample from. Furthermore, we also implemented a technique to use *two levels* of the mipmap, since moving between each mip level could otherwise look jarring. In this case, for each screen sample we take, we actually take *two* (rather than one) texture space samples from two adjacent mipmap levels (derived from our normal mipmap level calculation). We then perform bilinear interpolation between the two to get our final texture space sample. This "combination" of two texture space samples roughly approximates continuous mipmap levels.

I found this project really showed me the complexity of drawing something to a frame buffer: it wasn’t something I had necessarily realized would require testing and as much processing as it did from the computer. One of the most interesting parts of the project for me was texturing: I found the concept of sampling a screen point that we had *already* decided to rasterize into texture space really cool and I enjoyed dealing with the challenges it brought. It was particularly enjoyable to try and solve the challenge of choosing which mipmap level to choose and interpolating between mipmap levels in order to try and reduce aliasing.

